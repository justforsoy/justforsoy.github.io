{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: 什么是高斯过程\n",
    "date: '2024-01-04'\n",
    "categories:\n",
    "  - 随机过程\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们经常遇到这样的推断数据：\n",
    "\n",
    "$$\\mathcal{D} = \\{(\\boldsymbol{x_i},y_i)|i=1,\\ldots,n, \\boldsymbol{x_i} \\in \\mathcal{X}, y_i \\in \\mathbb{R}\\}$$\n",
    "\n",
    "其中 $\\boldsymbol{x_i}$ 是输入，而 $y_i$ 是目标输出，我们希望预测新的输入 $\\boldsymbol{x_*}$ 的输出 $y_*$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从贝叶斯的角度来看，我们可以构建一个模型来定义所有可能函数的分布$(f: \\mathcal{X} \\rightarrow \\mathbb{R})$。我们可以将初始信念编码为这些函数的先验。基于观测数据和贝叶斯定理来推断后验分布。然后基于后验得到输入 $\\boldsymbol{x_*}$ 的对 $y_*$ 的预测分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "高斯过程 Gaussian processes (GPs) 是一种函数的概率分布。对于这种分布，我们可以进行可行的推断。    \n",
    "它们可以被视为高斯概率分布到函数空间的推广。即多元高斯分布定义了有限随机变量集的分布，高斯过程定义了无限随机变量集（例如实数）的分布。GP域不限于实数，任何具有点积的空间都适用。    \n",
    "与多元高斯分布类似，GP 由其均值 $\\mu$ 和协方差 $k$ 定义。但是对于 GP，这些是函数，而不是向量，即 $\\mu: \\mathcal{X} \\rightarrow \\mathbb{R}$ 和 $k: \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$。      \n",
    "为方便又不是一般性，下面说明中我们假设均值为零，即 $\\mu(\\boldsymbol{x}) = 0$。\n",
    "\n",
    "![](samples_from_prior.png)\n",
    "\n",
    "![](samples_from_posterior.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上图来自具有相同均值和协方差函数的两个高斯过程的样本。这里有 $\\mathcal{X} = \\mathbb{R}$。先验样本来自没有任何数据的高斯过程，后验样本取自有观测数据高斯过程，其中数据显示为黑色方块。黑色虚线代表过程的平均值，灰色阴影区域覆盖每个输入的标准偏差的两倍。彩色线条是来自过程的样本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成样本的代码如下：\n",
    "\n",
    "```python\n",
    "from numpy.random import seed\n",
    "from infpy.gp import GaussianProcess, gp_1D_X_range, gp_plot_samples_from\n",
    "from pylab import plot, savefig, title, close, figure, xlabel, ylabel\n",
    "\n",
    "# seed RNG to make reproducible and close all existing plot windows\n",
    "seed(2)\n",
    "close('all')\n",
    "\n",
    "#\n",
    "# Kernel\n",
    "#\n",
    "from infpy.gp import SquaredExponentialKernel as SE\n",
    "kernel = SE([1])\n",
    "\n",
    "#\n",
    "# Part of X-space we will plot samples from\n",
    "#\n",
    "support = gp_1D_X_range(-10.0, 10.01, .125)\n",
    "\n",
    "#\n",
    "# Plot samples from prior\n",
    "#\n",
    "figure()\n",
    "gp = GaussianProcess([], [], kernel)\n",
    "gp_plot_samples_from(gp, support, num_samples=3)\n",
    "xlabel('x')\n",
    "ylabel('f(x)')\n",
    "title('Samples from the prior')\n",
    "savefig('samples_from_prior.png')\n",
    "savefig('samples_from_prior.eps')\n",
    "\n",
    "#\n",
    "# Data\n",
    "#\n",
    "X = [[-5.], [-2.], [3.], [3.5]]\n",
    "Y = [2.5, 2, -.5, 0.]\n",
    "\n",
    "#\n",
    "# Plot samples from posterior\n",
    "#\n",
    "figure()\n",
    "plot([x[0] for x in X], Y, 'ks')\n",
    "gp = GaussianProcess(X, Y, kernel)\n",
    "gp_plot_samples_from(gp, support, num_samples=3)\n",
    "xlabel('x')\n",
    "ylabel('f(x)')\n",
    "title('Samples from the posterior')\n",
    "savefig('samples_from_posterior.png')\n",
    "savefig('samples_from_posterior.eps')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 协方差函数 $k$\n",
    "\n",
    "假设均值函数为0，则 GP 由观测数据 $\\mathcal{D}$ 和 协方差函数 $k$ 决定。观测数据是确定的，因此我们只需要定义协方差函数。    \n",
    "幸运的是，我们有一个庞大的可能协方差函数库，每个协方差函数代表函数空间上的不同先验。    \n",
    "\n",
    "![](covariance_function_se.png)\n",
    "\n",
    "![](covariance_function_matern_52.png)\n",
    "\n",
    "![](covariance_function_periodic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从具有相同数据和不同协方差函数的 GP 中抽样。不同协方差函数的 GP 后验的样本具有不同的特征。periodic 协方差函数的主要特征是具有周期性。其他协方差函数以不同的方式影响样本的平滑度。\n",
    "\n",
    "产生样本的代码如下：\n",
    "\n",
    "```python\n",
    "from numpy.random import seed\n",
    "from infpy.gp import GaussianProcess, gp_1D_X_range, gp_plot_samples_from\n",
    "from pylab import plot, savefig, title, close, figure, xlabel, ylabel\n",
    "from infpy.gp import SquaredExponentialKernel as SE\n",
    "from infpy.gp import Matern52Kernel as Matern52\n",
    "from infpy.gp import Matern52Kernel as Matern32\n",
    "from infpy.gp import RationalQuadraticKernel as RQ\n",
    "from infpy.gp import NeuralNetworkKernel as NN\n",
    "from infpy.gp import FixedPeriod1DKernel as Periodic\n",
    "from infpy.gp import noise_kernel as noise\n",
    "\n",
    "# seed RNG to make reproducible and close all existing plot windows\n",
    "seed(2)\n",
    "close('all')\n",
    "\n",
    "#\n",
    "# Part of X-space we will plot samples from\n",
    "#\n",
    "support = gp_1D_X_range(-10.0, 10.01, .125)\n",
    "\n",
    "#\n",
    "# Data\n",
    "#\n",
    "X = [[-5.], [-2.], [3.], [3.5]]\n",
    "Y = [2.5, 2, -.5, 0.]\n",
    "\n",
    "def plot_for_kernel(kernel, fig_title, filename):\n",
    "  figure()\n",
    "  plot([x[0] for x in X], Y, 'ks')\n",
    "  gp = GaussianProcess(X, Y, kernel)\n",
    "  gp_plot_samples_from(gp, support, num_samples=3)\n",
    "  xlabel('x')\n",
    "  ylabel('f(x)')\n",
    "  title(fig_title)\n",
    "  savefig('%s.png' % filename)\n",
    "  savefig('%s.eps' % filename)\n",
    "  \n",
    "plot_for_kernel(\n",
    "  kernel=Periodic(6.2),\n",
    "  fig_title='Periodic',\n",
    "  filename='covariance_function_periodic'\n",
    ")\n",
    "\n",
    "plot_for_kernel(\n",
    "  kernel=RQ(1., dimensions=1),\n",
    "  fig_title='Rational quadratic',\n",
    "  filename='covariance_function_rq'\n",
    ")\n",
    "\n",
    "plot_for_kernel(\n",
    "  kernel=SE([1]),\n",
    "  fig_title='Squared exponential',\n",
    "  filename='covariance_function_se'\n",
    ")\n",
    "\n",
    "plot_for_kernel(\n",
    "  kernel=SE([3.]),\n",
    "  fig_title='Squared exponential (long length scale)',\n",
    "  filename='covariance_function_se_long_length'\n",
    ")\n",
    "\n",
    "plot_for_kernel(\n",
    "  kernel=Matern52([1.]),\n",
    "  fig_title='Matern52',\n",
    "  filename='covariance_function_matern_52'\n",
    ")\n",
    "\n",
    "plot_for_kernel(\n",
    "  kernel=Matern32([1.]),\n",
    "  fig_title='Matern32',\n",
    "  filename='covariance_function_matern_32'\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 协方差函数和噪音数据的结合\n",
    "\n",
    "此外，协方差函数的逐点乘积和本身就是协方差函数。通过这种方式，我们可以组合简单的协方差函数来表示我们对函数的更复杂的信念。    \n",
    "通常我们正在对一个我们实际上无法获取的目标值 $y$ 建模，我们只有它的有噪音版本 $y+\\epsilon$。    \n",
    "如果我们假设 $\\epsilon$ 是一个方差为 $\\sigma_n^2$ 的高斯分布，我们就能将噪音合并进协方差函数中。    \n",
    "我们的带噪声的高斯过程（noisy GP）的协方差函数 $k_{\\textrm{noise}}(x_1,x_2)$ 需要能够识别输入 $x_1$ 和 $x_2$ 是否相同，因为我们可能在 $\\mathcal{X}$ 的同一点有两个带噪声的测量值。\n",
    "$$k_{\\textrm{noise}}(x_1,x_2) = k(x_1,x_2) + \\delta(x_1=x_2) \\sigma_n^2\\$$\n",
    "\n",
    "![](noise_low.png)\n",
    "![](noise_mid.png)\n",
    "![](noise_high.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成上图的代码：\n",
    "    \n",
    "```python\n",
    "from numpy.random import seed\n",
    "from infpy.gp import GaussianProcess, gp_1D_X_range, gp_plot_prediction\n",
    "from pylab import plot, savefig, title, close, figure, xlabel, ylabel\n",
    "from infpy.gp import SquaredExponentialKernel as SE\n",
    "from infpy.gp import noise_kernel as noise\n",
    "\n",
    "# close all existing plot windows\n",
    "close('all')\n",
    "\n",
    "#\n",
    "# Part of X-space we are interested in\n",
    "#\n",
    "support = gp_1D_X_range(-10.0, 10.01, .125)\n",
    "\n",
    "#\n",
    "# Data\n",
    "#\n",
    "X = [[-5.], [-2.], [3.], [3.5]]\n",
    "Y = [2.5, 2, -.5, 0.]\n",
    "\n",
    "def plot_for_kernel(kernel, fig_title, filename):\n",
    "  figure()\n",
    "  plot([x[0] for x in X], Y, 'ks')\n",
    "  gp = GaussianProcess(X, Y, kernel)\n",
    "  mean, sigma, LL = gp.predict(support)\n",
    "  gp_plot_prediction(support, mean, sigma)\n",
    "  xlabel('x')\n",
    "  ylabel('f(x)')\n",
    "  title(fig_title)\n",
    "  savefig('%s.png' % filename)\n",
    "  savefig('%s.eps' % filename)\n",
    "  \n",
    "plot_for_kernel(\n",
    "  kernel=SE([1.]) + noise(.1),\n",
    "  fig_title='k = SE + noise(.1)',\n",
    "  filename='noise_mid'\n",
    ")\n",
    "\n",
    "plot_for_kernel(\n",
    "  kernel=SE([1.]) + noise(1.),\n",
    "  fig_title='k = SE + noise(1)',\n",
    "  filename='noise_high'\n",
    ")\n",
    "\n",
    "plot_for_kernel(\n",
    "  kernel=SE([1.]) + noise(.0001),\n",
    "  fig_title='k = SE + noise(.0001)',\n",
    "  filename='noise_low'\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学习协方差函数参数\n",
    "\n",
    "大多数常用的协方差函数都是参数化的。如果我们对问题的理解有信心，则可以使用固定参数。或者我们可以将它们视为贝叶斯推理任务中的超参数，并通过最大似然估计或共轭梯度下降等技术对其进行优化。\n",
    "\n",
    "![](learning_first_guess.png)\n",
    "![](learning_learnt.png)\n",
    "\n",
    "\n",
    "我们看到第二张图中的预测似乎更准确地拟合数据，这是学习超参数的效果。\n",
    "\n",
    "生成上图的代码：\n",
    "\n",
    "```python\n",
    "from numpy.random import seed\n",
    "from infpy.gp import GaussianProcess, gp_1D_X_range\n",
    "from infpy.gp import gp_plot_prediction, gp_learn_hyperparameters\n",
    "from pylab import plot, savefig, title, close, figure, xlabel, ylabel\n",
    "from infpy.gp import SquaredExponentialKernel as SE\n",
    "from infpy.gp import noise_kernel as noise\n",
    "\n",
    "# close all existing plot windows\n",
    "close('all')\n",
    "\n",
    "#\n",
    "# Part of X-space we are interested in\n",
    "#\n",
    "support = gp_1D_X_range(-10.0, 10.01, .125)\n",
    "\n",
    "#\n",
    "# Data\n",
    "#\n",
    "X = [[-5.], [-2.], [3.], [3.5]]\n",
    "Y = [2.5, 2, -.5, 0.]\n",
    "\n",
    "def plot_gp(gp, fig_title, filename):\n",
    "  figure()\n",
    "  plot([x[0] for x in X], Y, 'ks')\n",
    "  mean, sigma, LL = gp.predict(support)\n",
    "  gp_plot_prediction(support, mean, sigma)\n",
    "  xlabel('x')\n",
    "  ylabel('f(x)')\n",
    "  title(fig_title)\n",
    "  savefig('%s.png' % filename)\n",
    "  savefig('%s.eps' % filename)\n",
    "  \n",
    "#\n",
    "# Create a kernel with reasonable parameters and plot the GP predictions\n",
    "#\n",
    "kernel = SE([1.]) + noise(1.)\n",
    "gp = GaussianProcess(X, Y, kernel)\n",
    "plot_gp(\n",
    "  gp=gp,\n",
    "  fig_title='Initial parameters: kernel = SE([1]) + noise(1)',\n",
    "  filename='learning_first_guess'\n",
    ")\n",
    "\n",
    "#\n",
    "# Learn the covariance function's parameters and replot\n",
    "#\n",
    "gp_learn_hyperparameters(gp)\n",
    "plot_gp(\n",
    "  gp=gp,\n",
    "  fig_title='Learnt parameters: kernel = SE([%.2f]) + noise(%.2f)' % (\n",
    "    kernel.k1.params[0],\n",
    "    kernel.k2.params.o2[0]\n",
    "  ),\n",
    "  filename='learning_learnt'\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考资料\n",
    "[What are Gaussian processes?](https://pythonhosted.org/infpy/gps.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
